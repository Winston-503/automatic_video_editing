{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic video cutter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import wave\n",
    "import json\n",
    "\n",
    "import moviepy.editor as mp\n",
    "from vosk import Model, KaldiRecognizer, SetLogLevel\n",
    "# to install with jupyter uncomment and run next cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install moviepy vosk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a vosk model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading your vosk model 'models/vosk-model-ru-0.10'...\n",
      "'models/vosk-model-ru-0.10' model was successfully read\n"
     ]
    }
   ],
   "source": [
    "# path to vosk model downloaded from\n",
    "# https://alphacephei.com/vosk/models\n",
    "model_path = \"models/vosk-model-ru-0.10\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(\"Please download the model from\" +\n",
    "          f\"https://alphacephei.com/vosk/models and unpack as {model_path}\")\n",
    "    sys.exit()\n",
    "\n",
    "print(f\"Reading your vosk model '{model_path}'...\")\n",
    "model = Model(model_path)\n",
    "print(f\"'{model_path}' model was successfully read\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set video filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to video file to convert\n",
    "# any extensions supported by ffmpeg: \n",
    "# .ogv, .mp4, .mpeg, .avi, .mov, .mkv etc.\n",
    "video_path = \"videos/test.mp4\"\n",
    "# new filename to save final video\n",
    "result_path = video_path[:-4] + \"_processed.mp4\"\n",
    "\n",
    "# temporary filename for audiofile (will be deleted)\n",
    "audio_path = video_path[:-3] + \"wav\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read video and convert it to mono audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if videofile exists\n",
    "if not os.path.exists(video_path):\n",
    "    print(f\"File {video_path} doesn't exist\")\n",
    "    sys.exit()\n",
    "\n",
    "# read video\n",
    "clip = mp.VideoFileClip(video_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:  39%|███▉      | 581/1479 [00:00<00:00, 3027.10it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in videos/test.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# convert video to audio\n",
    "# ffmpeg_params=[\"-ac\", \"1\"] parameter convert audio to mono format\n",
    "clip.audio.write_audiofile(audio_path, ffmpeg_params=[\"-ac\", \"1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Recognition with vosk model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_audio_vosk(audio_path, model):\n",
    "    '''\n",
    "    Recognize audio with vosk model.\n",
    "    Language of the recognition depends on model.\n",
    "    Returns list of JSON dictionaries. Each of them has the following structure:\n",
    "\n",
    "    {'result': [{'conf': 0.849133, # confidence\n",
    "                 'end': 4.5, # end time\n",
    "                 'start': 4.05, # start time\n",
    "                 'word': 'test'}, # recognized word\n",
    "\n",
    "                 # the same for other words in sentence\n",
    "                 ], \n",
    "     'text': 'test'}\n",
    "\n",
    "    Parameters:\n",
    "        audio_path (str): path to the audio file to recognize. Must be WAV format mono PCM\n",
    "        model: vosk model. Must be loaded with `model = Model(model_path)` command\n",
    "\n",
    "    Returns:\n",
    "        results (array): list of JSON dictionaries from vosk model\n",
    "    '''\n",
    "\n",
    "    # check if audio is mono wav\n",
    "    wf = wave.open(audio_path, \"rb\")\n",
    "    if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getcomptype() != \"NONE\":\n",
    "        print(\"Audio file must be WAV format mono PCM\")\n",
    "        sys.exit()\n",
    "\n",
    "    rec = KaldiRecognizer(model, wf.getframerate())\n",
    "    rec.SetWords(True)\n",
    "\n",
    "    print('Starting to convert audio to text. It may take some time...')\n",
    "    start_time = time.time()\n",
    "\n",
    "    results = []\n",
    "    # recognize speech using vosk model\n",
    "    while True:\n",
    "        data = wf.readframes(4000)\n",
    "        if len(data) == 0:\n",
    "            break\n",
    "        if rec.AcceptWaveform(data):\n",
    "            part_result = json.loads(rec.Result())\n",
    "            results.append(part_result)\n",
    "\n",
    "    part_result = json.loads(rec.FinalResult())\n",
    "    results.append(part_result)\n",
    "\n",
    "    # forming a final string from the words\n",
    "    text = ''\n",
    "    for r in results:\n",
    "        text += r['text'] + ' '\n",
    "\n",
    "    time_elapsed = time.strftime('%H:%M:%S',\n",
    "                                 time.gmtime(time.time() - start_time))\n",
    "    print(f'Done! Elapsed time = {time_elapsed}')\n",
    "\n",
    "    print(\"\\tVosk thinks you said:\\n\")\n",
    "    print(text)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to convert audio to text. It may take some time...\n",
      "Done! Elapsed time = 00:00:21\n",
      "\tVosk thinks you said:\n",
      "\n",
      "баре видео и меняю крэша анал и мелтон мэтью петра ноутбук ты шёл и начало на с м оперся на алтарь бишон видео с рулетками юпитера ноутбук    круиз не пайтон скрыт ы не конец сел но вами а вот у ноутбука фронт то что волны не то импорт сам пайтон \n"
     ]
    }
   ],
   "source": [
    "results = recognize_audio_vosk(audio_path=audio_path, \n",
    "                               model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete audio\n",
    "os.remove(audio_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for control words and timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segments_from_audio(results, start_word='начало', end_word='конец', offset=0.5):\n",
    "    '''\n",
    "    Parse list of JSON dictionaries for 'start_word' and 'end_word' \n",
    "    and returns 'segments' - list of tuples, where each turple is\n",
    "    (start_time of start_word - offset, end_time of end_word + offset)\n",
    "\n",
    "    Parameters:\n",
    "        results (array): list of JSON dictionaries from vosk model. \n",
    "                         Received from `recognize_audio_vosk()` function\n",
    "        start_word (str): control word that signals the beginning of the video fragment to be cut\n",
    "        end_word (str): control word that signals the ending of the video fragment to be cut\n",
    "        offset (float): offset in seconds. Number being subtracted from 'start_time' for 'start_word' \n",
    "                        and added to 'end_time' for 'end_word'\n",
    "\n",
    "    Returns:\n",
    "        segments (array): list of tuples (start_time, end_time)\n",
    "    '''\n",
    "\n",
    "    print(\"Starting the search for control words...\")\n",
    "\n",
    "    # lists for start and end times\n",
    "    starts = []\n",
    "    ends = []\n",
    "\n",
    "    for record in results:\n",
    "        if start_word in record['text'] or end_word in record['text']:\n",
    "            # the sentence contains 'start_word' or 'end_word'\n",
    "            for word_object in record['result']:\n",
    "                # cycle by words in a sentence\n",
    "                if word_object['word'] == start_word:\n",
    "                    starts.append(word_object['start'] - offset)\n",
    "                if word_object['word'] == end_word:\n",
    "                    ends.append(word_object['end'] + offset)\n",
    "\n",
    "    # from starts and ends to segments\n",
    "    # starts = [1, 3], ends = [2, 4] ->\n",
    "    # segments = (0, 1), (2, 3), (4, None)\n",
    "\n",
    "    segments = []\n",
    "    length = max(len(starts), len(ends))\n",
    "    for i in range(length + 1):\n",
    "        if i == 0:\n",
    "            segments.append((0, starts[0]))\n",
    "        elif i == length:\n",
    "            segments.append((ends[i-1], None))\n",
    "        else:\n",
    "            # intermediate values\n",
    "            segments.append((ends[i-1], starts[i]))\n",
    "    print(\"The search of control words is completed. Got the following array of segments: \\n\")\n",
    "    print(segments)\n",
    "\n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the search for control words...\n",
      "The search of control words is completed. Got the following array of segments: \n",
      "\n",
      "[(0, 11.65), (57.56, None)]\n"
     ]
    }
   ],
   "source": [
    "segments = get_segments_from_audio(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_video_by_segments(video, segments, result_path):\n",
    "    '''\n",
    "    Crop video according to 'segments' list and\n",
    "    save final video to 'result_path'\n",
    "\n",
    "    Parameters:\n",
    "        video: moviepy.editor.VideoFileClip object\n",
    "        segments (array): list of tuples (start_time, end_time).\n",
    "                          Received from `get_segments_from_audio()` function\n",
    "        result_path (str): new filename to save final video\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "\n",
    "    print(\"Starting the video processing...\")\n",
    "\n",
    "    clips = []  # list of all video fragments\n",
    "\n",
    "    for start_seconds, end_seconds in segments:\n",
    "        # crop a video clip and add it to list\n",
    "        c = video.subclip(start_seconds, end_seconds)\n",
    "        clips.append(c)\n",
    "\n",
    "    final_clip = mp.concatenate_videoclips(clips)\n",
    "    final_clip.write_videofile(result_path)\n",
    "\n",
    "    print(\"The video processing is completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the video processing...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:  52%|█████▏    | 243/467 [00:00<00:00, 1343.45it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video videos/test_processed.mp4.\n",
      "MoviePy - Writing audio in test_processedTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   3%|▎         | 16/635 [00:00<00:03, 156.86it/s, now=None]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video videos/test_processed.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                               \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready videos/test_processed.mp4\n",
      "The video processing is completed\n"
     ]
    }
   ],
   "source": [
    "crop_video_by_segments(video=clip, \n",
    "                       segments=segments,\n",
    "                       result_path=result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
