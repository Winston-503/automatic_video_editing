{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Automatic video cutter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://alphacephei.com/vosk/\n",
    "\n",
    "\n",
    "https://github.com/alphacep/vosk-api\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install nbextension for jupyter notebook\n",
    "# !pip install jupyter_contrib_nbextensions && jupyter contrib nbextension install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import wave\n",
    "import json\n",
    "\n",
    "import moviepy.editor as mp\n",
    "from vosk import Model, KaldiRecognizer, SetLogLevel\n",
    "# !pip install moviepy vosk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a vosk model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading your vosk model 'models/vosk-model-ru-0.10'...\n",
      "'models/vosk-model-ru-0.10' model was successfully read\n"
     ]
    }
   ],
   "source": [
    "# path to vosk model downloaded from\n",
    "# https://alphacephei.com/vosk/models\n",
    "\n",
    "# vosk-model-ru-0.10\n",
    "# vosk-model-en-us-0.21\n",
    "model_path = \"models/vosk-model-ru-0.10\"\n",
    "\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"Please download the model from https://alphacephei.com/vosk/models and unpack as {model_path}\")\n",
    "    sys.exit()\n",
    "\n",
    "print(f\"Reading your vosk model '{model_path}'...\")\n",
    "model = Model(model_path)\n",
    "print(f\"'{model_path}' model was successfully read\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set video filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to video file to convert\n",
    "# extensions supported:\n",
    "# \n",
    "video_path = \"videos/2021-10-01_12-56-02.mkv\"\n",
    "# new video filename to save\n",
    "result_path = video_path[:-4] + \"_processed.mp4\" #+ # video_path[-3:]\n",
    "\n",
    "audio_path = video_path[:-3] + \"wav\"\n",
    "# text_path = video_path[:-3] + \"txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read video and convert to mono audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read video\n",
    "clip = mp.VideoFileClip(video_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:  22%|██▏       | 204/911 [00:00<00:00, 1946.75it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in videos/2021-10-01_12-56-02.wav\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# convert video to audio\n",
    "# ffmpeg_params=[\"-ac\", \"1\"] parameter convert audio to mono format\n",
    "clip.audio.write_audiofile(audio_path, ffmpeg_params=[\"-ac\", \"1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Speech Recognition with vosk model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_audio_vosk(audio_path, model):\n",
    "    '''\n",
    "    Recognize audio with vosk model.\n",
    "    Language of the recognition depends on vosk model.\n",
    "\n",
    "    Parameters:\n",
    "        audio_path (str): path to the audio file to recognize. Must be WAV format mono PCM\n",
    "        model: vosk model. Must be loaded with `model = Model(model_path)` command\n",
    "\n",
    "    Returns:\n",
    "        results (array): list of \n",
    "    '''\n",
    "    # check if audio if mono wav\n",
    "    wf = wave.open(audio_path, \"rb\")\n",
    "    if wf.getnchannels() != 1 or wf.getsampwidth() != 2 or wf.getcomptype() != \"NONE\":\n",
    "        print(\"Audio file must be WAV format mono PCM.\")\n",
    "        sys.exit()\n",
    "\n",
    "    rec = KaldiRecognizer(model, wf.getframerate())\n",
    "    rec.SetWords(True)\n",
    "\n",
    "    print('Starting to convert audio to text. It may take some time...')\n",
    "    start_time = time.time()\n",
    "\n",
    "    results = []\n",
    "    # recognize speech using vosk model\n",
    "    while True:\n",
    "        data = wf.readframes(4000)\n",
    "        if len(data) == 0:\n",
    "            break\n",
    "        if rec.AcceptWaveform(data):\n",
    "            part_result = json.loads(rec.Result())\n",
    "            results.append(part_result)\n",
    "\n",
    "    part_result = json.loads(rec.FinalResult())\n",
    "    results.append(part_result)\n",
    "\n",
    "    # forming a final string from the words\n",
    "    text = ''\n",
    "    for r in results:\n",
    "        text += r['text'] + ' '\n",
    "\n",
    "    time_elapsed = time.strftime('%H:%M:%S',\n",
    "                                 time.gmtime(time.time() - start_time))\n",
    "    print(f'Done! Elapsed time = {time_elapsed}')\n",
    "\n",
    "    print(\"\\tVosk thinks you said:\\n\")\n",
    "    print(text)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to convert audio to text. It may take some time...\n",
      "Done! Elapsed time = 00:00:20\n",
      "\tVosk thinks you said:\n",
      "\n",
      "в баре огонь ток болт вы не выкрашен бла бла бла бла бла бла как микс от центра уэйд сми окон мае че патруль ноутбук тушу льюсом кот туши луисом кор начало весь мой персонал не тратят это тает трубы клайд антенн сей конец хайкен и и и а потом \n"
     ]
    }
   ],
   "source": [
    "results = recognize_audio_vosk(audio_path=audio_path, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# delete audio\n",
    "\n",
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search for control words and timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segments_from_audio(results, start_word='начало', end_word='конец', offset=0.5):\n",
    "    '''\n",
    "    \n",
    "    Parameters:\n",
    "        results (array): Received from `recognize_audio_vosk()` function\n",
    "        start_word (str): control word that signals the beginning of the video fragment to be cut\n",
    "        end_word (str):   control word that signals the ending of the video fragment to be cut\n",
    "        offset (float): offset in seconds. Number being subtracted from 'start_time' for 'start_word' \n",
    "                        and added to 'end_time' for 'end_word'\n",
    "                        \n",
    "    Returns:\n",
    "        segments (array): list of tuples of two elements\n",
    "    '''\n",
    "    \n",
    "    print(\"Starting the search for control words...\")\n",
    "    # lists for start and end times\n",
    "    starts = []\n",
    "    ends = []\n",
    "    \n",
    "    for record in results:\n",
    "        # print(record['text'])\n",
    "        if start_word in record['text'] or end_word in record['text']:\n",
    "            # the sentence contains 'start_word' or 'end_word'\n",
    "            for word_object in record['result']:\n",
    "                # cycle by words in a sentence\n",
    "                if word_object['word'] == start_word:\n",
    "                    starts.append(word_object['start'] - offset)\n",
    "                if word_object['word'] == end_word:\n",
    "                    ends.append(word_object['end'] + offset)\n",
    "                \n",
    "    # from starts and ends to segments\n",
    "    # starts = [1, 3], ends = [2, 4] ->\n",
    "    # segments = (0, 1), (2, 3), (4, None)\n",
    "\n",
    "    segments = []\n",
    "    length = max(len(starts), len(ends))\n",
    "    for i in range(length + 1):\n",
    "        if i == 0:\n",
    "            segments.append((0, starts[0]))\n",
    "        elif i == length:\n",
    "            segments.append((ends[i-1], None))\n",
    "        else:\n",
    "            # intermediate values\n",
    "            segments.append((ends[i-1], starts[i]))\n",
    "    print(\"The search of control words is completed. Got the following array of segments: \\n\")\n",
    "    print(segments)\n",
    "    \n",
    "    return segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting the search for control words...\n",
      "The search of control words is completed. Got the following array of segments: \n",
      "\n",
      "[(0, 22.3), (34.34, None)]\n"
     ]
    }
   ],
   "source": [
    "segments = get_segments_from_audio(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_video_by_segments(video, segments, result_path, verbose=False):\n",
    "    '''\n",
    "    \n",
    "    Parameters:\n",
    "        video :\n",
    "        segments : Received from `get_segments_from_audio()` function\n",
    "        verbose (bool): Default False.\n",
    "        \n",
    "    Returns:\n",
    "        None\n",
    "    '''\n",
    "    \n",
    "    clips = [] # list of all video fragments\n",
    "\n",
    "    for start_seconds, end_seconds in segments:\n",
    "        # crop a video clip and add it to list\n",
    "        c = video.subclip(start_seconds, end_seconds)\n",
    "        clips.append(c)\n",
    "\n",
    "    final_clip = mp.concatenate_videoclips(clips)\n",
    "    final_clip.write_videofile(result_path)\n",
    "    \n",
    "    if verbose:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "chunk:   0%|          | 2/645 [00:00<00:37, 17.25it/s, now=None]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video videos/2021-10-01_12-56-02_processed.mp4.\n",
      "MoviePy - Writing audio in 2021-10-01_12-56-02_processedTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t:   1%|          | 9/1754 [00:00<00:19, 89.96it/s, now=None]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video videos/2021-10-01_12-56-02_processed.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready videos/2021-10-01_12-56-02_processed.mp4\n"
     ]
    }
   ],
   "source": [
    "crop_video_by_segments(video=clip, segments=segments, result_path=result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
